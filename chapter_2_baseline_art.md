## 第二章：基準線 (Baseline) 的藝術：高效變更檢測的基石 (Chapter 2: The Art of the Baseline: The Cornerstone of Efficient Change Detection)

在 Excel Monitor 的世界裡，如果說「變更檢測」是其核心使命，那麼「基準線」無疑就是實現這一使命的基石。您可以將基準線想像成一個 Excel 檔案在某個特定時間點的**「精確快照」**或**「數字指紋」**。它不僅僅是一個檔案的簡單複製，它是一個經過精心設計、高度優化且具有生命週期的資料結構。本章將深入探討基準線的設計哲學、生命週期管理以及背後的關鍵技術，揭示它是如何讓系統能夠高效、準確地捕捉到 Excel 檔案的每一次細微變化的。

### 2.1 什麼是基準線？

從本質上講，一個基準線檔案是一個**JSON (JavaScript Object Notation) 物件**。JSON 是一種輕量級的數據交換格式，它易於人閱讀和編寫，也易於機器解析和生成。這使得基準線檔案既能被程式高效處理，也能在必要時被人類理解。

這個 JSON 物件以一種結構化的方式，完整地記錄了一個 Excel 檔案在某個特定時間點的**所有關鍵內容**。您可以把它想像成一個 Excel 檔案的「數據骨架」，只保留了我們關心的核心信息。

一個典型的基準線檔案 (`<檔案名稱>.baseline.json.<壓縮格式>`) 包含以下核心資訊：

```json
{
  "last_author": "Michael Cheng",
  "content_hash": "a1b2c3d4e5f6a7b8c9d0e1f2a3b4c5d6",
  "cells": {
    "Sheet1": {
      "A1": {"formula": null, "value": "Sales"},
      "B1": {"formula": null, "value": 1500},
      "C1": {"formula": "=B1*1.2", "value": 1800}
    },
    "Sheet2": {
      "A1": {"formula": null, "value": "Expenses"},
      "B1": {"formula": null, "value": 800}
    }
  },
  "timestamp": "2023-10-27T10:30:00.123456"
}
```

讓我們逐一解釋這些欄位的作用：

*   **`last_author` (最後修改作者)**：
    *   **作用**：這個欄位記錄了在生成這個基準線時，Excel 檔案的「屬性」中顯示的最後修改者名稱。這對於審計追蹤至關重要，因為它能幫助我們回答「誰動了我的 Excel？」這個問題。
    *   **重要性**：在企業環境中，追溯變更的責任人是數據治理和合規性的基本要求。

*   **`content_hash` (內容雜湊值)**：
    *   **作用**：這是整個 `cells` 物件內容的 MD5 雜湊值。您可以把它想像成 `cells` 數據的一個**獨特且簡短的「指紋」**。即使 `cells` 數據非常龐大，這個 `content_hash` 始終是一個固定長度的字串（MD5 雜湊值是 32 個字元的十六進制字串）。
    *   **重要性**：它是系統能夠**快速判斷檔案是否有實質變更**的核心優化手段。我們將在 2.3 節詳細解釋其工作原理。

*   **`cells` (儲存格數據)**：
    *   **作用**：這是基準線的**主體**，包含了 Excel 檔案中所有被監控的儲存格的詳細數據。它是一個巢狀的數據結構，設計得非常巧妙：
        *   **第一層鍵**：是工作表 (Worksheet) 的名稱（例如 `"Sheet1"`, `"Sheet2"`）。這使得我們可以按工作表來組織和存取數據。
        *   **第二層鍵**：是儲存格的位址 (e.g., `"A1"`, `"C1"`)。這使得我們可以精確定位到每一個儲存格。
        *   **最內層物件**：則儲存了該儲存格的兩個關鍵屬性：
            *   `"formula"` (公式)：如果儲存格包含公式，這裡會記錄公式的字串形式（例如 `"=B1*1.2"`）。如果沒有公式，則為 `null`。
            *   `"value"` (值)：這裡記錄了儲存格的**計算結果或直接輸入的值**。例如，如果公式是 `=B1*1.2` 且 B1 是 1500，那麼 `value` 就是 1800。
    *   **重要性**：這個結構確保了我們能夠捕捉到 Excel 檔案中**最核心的變更信息**：公式的變化（業務邏輯的變化）和數值的變化（數據本身的變化）。

*   **`timestamp` (時間戳)**：
    *   **作用**：記錄了該基準線建立時的精確時間。它標識了這個快照是何時被「拍攝」的。
    *   **重要性**：為每一次變更提供了時間維度，是構建完整審計日誌的基礎。

### 2.2 基準線的生命週期

基準線在其存在期間，會經歷一個清晰的生命週期，由 `core/baseline.py` 模組進行管理。您可以將這個生命週期想像成一個檔案從誕生到成熟，再到最終歸檔的過程：

1.  **建立 (Creation)**：
    *   **觸發時機**：當一個新的 Excel 檔案首次被納入監控範圍，或者系統需要為一個現有檔案建立初始基準線時。
    *   **過程**：`create_baseline_for_files_robust` 函數會被呼叫。它會遍歷指定的 Excel 檔案列表。對於每一個檔案，它會執行以下操作：
        *   呼叫 `dump_excel_cells_with_timeout` (在 `core/excel_parser.py` 中) 來**深度解析**該 Excel 檔案的內容，提取出所有工作表、所有儲存格的公式和值。
        *   計算這些解析出來的數據的 `content_hash`（數字指紋）。
        *   將解析出的數據、計算出的雜湊值、最後修改作者和當前時間戳等信息，組合成一個完整的 JSON 物件。
        *   最後，通過 `save_baseline` 函數將這個 JSON 物件**寫入磁碟**，形成一個基準線檔案。
    *   **結果**：一個全新的基準線檔案誕生，代表了 Excel 檔案在某個時間點的「原始狀態」。

2.  **載入 (Loading)**：
    *   **觸發時機**：在系統需要對一個 Excel 檔案進行變更比對之前（例如，當檔案被修改時）。
    *   **過程**：`compare_excel_changes` 函數會首先呼叫 `load_baseline`。這個函數會根據 Excel 檔案的名稱，智能地找到對應的基準線檔案（無論其壓縮格式是什麼，系統都能自動識別）。它會讀取該基準線檔案的內容，並將其從壓縮格式解壓縮，然後從 JSON 格式反序列化為 Python 物件，以供後續的變更比對邏輯使用。
    *   **結果**：系統獲得了 Excel 檔案的「上一個已知穩定狀態」的數據，準備與當前狀態進行比對。

3.  **更新 (Updating)**：
    *   **觸發時機**：當 `compare_excel_changes` 函數確認 Excel 檔案發生了**有意義的變更**後。
    *   **過程**：系統會用當前讀取到的 Excel 檔案的最新內容，生成一個新的基準線資料物件（包含新的 `content_hash` 和 `timestamp`）。然後，它會再次呼叫 `save_baseline` 函數，將這個新的基準線物件寫入磁碟，**覆蓋掉**舊的基準線檔案。
    *   **重要性**：這個「覆蓋」操作至關重要。它代表了系統狀態的**推進**，將 Excel 檔案的「現在」變成了下一次比對的「過去」。這確保了基準線始終反映檔案的最新穩定狀態，為持續的變更追蹤提供了基礎。

4.  **歸檔 (Archiving)**：
    *   **觸發時機**：這是一個可選的、但非常智慧的生命週期階段。如果使用者在 `settings.py` 中啟用了 `ENABLE_ARCHIVE_MODE`，系統會定期（由 `ARCHIVE_AFTER_DAYS` 控制，例如每 7 天）檢查基準線資料夾。
    *   **過程**：`archive_old_baselines` 函數會像一個檔案管理員，巡視所有基準線檔案。對於那些長時間（超過 `ARCHIVE_AFTER_DAYS` 設定的天數）未被更新過的「冷」基準線檔案（通常是預設的、注重讀寫速度的壓縮格式，如 LZ4），系統會自動將其從當前格式解壓縮，然後再用**壓縮率更高**的歸檔格式（如 Zstd）重新壓縮。舊的、低壓縮率的檔案會被刪除。
    *   **重要性**：這個過程對使用者是完全透明的。其結果是，常用的、頻繁變更的檔案保持高速存取（使用 LZ4），而那些不常用的、歷史性的檔案則以最高效率儲存（使用 Zstd），實現了儲存資源的**動態最優化**。

### 2.3 內容雜湊 (Content Hashing)：秒級判斷的秘密

想像一下，如果每次檢查 Excel 檔案是否有變更時，都需要完整地讀取並比對一個 50MB 甚至 100MB 的大型 Excel 檔案，那將是一場**效能災難**。這會導致系統反應遲鈍，甚至可能因為頻繁的磁碟 I/O 而影響其他應用程式的運行。

`content_hash` 的引入，就是為了解決這個核心問題。它利用了**雜湊函數 (Hash Function)** 的特性。雜湊函數可以將任意長度的輸入（例如，一個 Excel 檔案的所有數據）轉換為一個固定長度的輸出（一個簡短的字串），這個輸出就是「雜湊值」或「指紋」。雜湊函數的關鍵特性是：

*   **唯一性**：即使輸入數據只有微小的變化，其雜湊值也會發生巨大的變化。這使得雜湊值成為數據內容的敏感「指紋」。
*   **不可逆性**：無法從雜湊值反推出原始數據。
*   **計算速度快**：計算雜湊值的過程非常高效。

在 Excel Monitor 中，`content_hash` 的工作原理如下：

1.  **生成指紋**：在建立基準線時，系統會使用 `hash_excel_content` 函數。這個函數會將包含所有儲存格數據的 `cells` 物件（一個 Python 字典）轉換為一個緊湊的 JSON 字串。然後，它會計算這個 JSON 字串的 **MD5 雜湊值**。這個 MD5 雜湊值（一個 32 個字元的十六進制字串）就成為了該檔案內容的唯一「指紋」。
2.  **快速比對**：在後續每次檢查 Excel 檔案是否有變更時，系統會先載入舊的基準線，取得其中儲存的 `old_hash`。然後，它會再次計算當前 Excel 檔案內容的 `curr_hash`。
3.  **跳過昂貴操作**：**在進行昂貴的逐一儲存格比對之前，系統只需比較這兩個簡短的雜湊值字串。**
    *   如果 `old_hash == curr_hash`，系統可以 100% 確定檔案的內容沒有發生任何實質性變更。這就像兩個指紋完全一樣，那麼這兩個人就是同一個人。此時，系統可以安全地**跳過**整個耗時的 Excel 解析和逐一儲存格比對過程，直接判斷為「無變更」。
    *   如果 `old_hash != curr_hash`，那麼系統就知道檔案內容肯定發生了變化，才需要進一步進行詳細的儲存格比對。

這是一個典型的**空間換時間**策略：通過在基準線檔案中增加一個極小的 `content_hash` 欄位（佔用極少的儲存空間），換來了在檢查更新時**數量級的效能提升**。對於監控大量 Excel 檔案的場景，這種優化是至關重要的，它使得系統能夠在極短的時間內判斷出哪些檔案需要進行詳細分析，哪些可以直接跳過。

### 2.4 智慧壓縮策略：速度與空間的藝術平衡

基準線檔案以 JSON 格式儲存，雖然可讀性好，但其純文字的特性導致儲存效率低下。一個大型 Excel 檔案的基準線，未經壓縮可能會佔用數十甚至數百 MB 的空間。這不僅浪費磁碟，更重要的是，頻繁地讀寫大型檔案會嚴重拖慢系統的反應速度。

因此，Excel Monitor 引入了一套智慧壓縮策略，由 `utils/compression.py` 模組實現，其核心思想是：**為不同的使用場景選擇最合適的壓縮演算法，以在速度和空間之間取得最佳平衡。**

#### 2.4.1 為何需要壓縮？

*   **節省磁碟空間**：JSON 文字數據通常具有很高的重複性，可以被高效地壓縮。通常能達到 80%-95% 的壓縮率，極大地減少了儲存成本。這對於長期儲存大量基準線檔案的場景尤為重要。
*   **加速 I/O 操作**：從磁碟讀取 1MB 的壓縮檔案，然後在記憶體中解壓縮成 10MB 的原始數據，遠比直接從磁碟讀取 10MB 的原始檔案要快。這是因為 CPU 解壓縮的時間開銷，通常遠小於磁碟 I/O 的時間節省。這對於提升基準線檔案的讀取速度，進而加速整個比對操作至關重要。

#### 2.4.2 LZ4 vs. Zstd vs. Gzip：演算法的選擇

專案內建了對三種主流壓縮演算法的支援，使用者可以通過 `settings.DEFAULT_COMPRESSION_FORMAT` 變數來設定預設行為。每種演算法都有其獨特的優勢和適用場景：

*   **LZ4 (`.lz4`)**：
    *   **特性**：**速度之王**。LZ4 的設計目標就是極致的壓縮和解壓縮速度。它的壓縮率雖然不是最高的，但其驚人的 I/O 吞吐量（每秒處理數 GB 的數據）使其在需要頻繁讀寫的場景中表現卓越。
    *   **適用場景**：非常適合處理那些**頻繁變更**的「熱」基準線檔案。在 Excel Monitor 中，這也是專案的**預設推薦**壓縮格式，因為它能確保最快的變更檢測響應速度。

*   **Zstandard (Zstd, `.zst`)**：
    *   **特性**：**平衡的典範**。Zstd 是由 Facebook 開發的現代壓縮演算法。它在提供接近高壓縮率（與 Gzip 相當甚至更好）的同時，依然保持了非常出色的壓縮和解壓縮速度。它在速度和壓縮率之間取得了極佳的平衡。
    *   **適用場景**：非常適合作為 `settings.ARCHIVE_COMPRESSION_FORMAT`，用於歸檔那些不經常變動的舊基準線。它能以較小的性能開銷，實現更高的儲存效率。

*   **Gzip (`.gz`)**：
    *   **特性**：**相容性的保障**。Gzip 是最廣為人知、普及度最高的壓縮格式，幾乎所有作業系統和工具都原生支援。雖然其速度和壓縮率在現代已經不算頂尖，但極佳的相容性使其成為一個可靠的備用選項。
    *   **適用場景**：當系統檢測到 LZ4 或 Zstd 函式庫不可用時（例如，某些環境可能沒有安裝這些函式庫），系統會自動降級到 Gzip，確保程式的核心功能不受影響。這是一種**容錯機制**。

#### 2.4.3 動態歸檔 (`ENABLE_ARCHIVE_MODE`)：智慧的生命週期管理

這是一個極具前瞻性的設計，它使得系統能夠根據基準線檔案的「活躍度」來動態調整其壓縮策略。系統不會靜態地對所有檔案使用同一種壓縮格式，而是像一個智慧的檔案管理員：

1.  **定期巡視**：`archive_old_baselines` 函數會像一個檔案管理員，定期巡視基準線資料夾。
2.  **識別「冷」數據**：當它發現一個基準線檔案（通常是 `.lz4` 格式）在 `settings.ARCHIVE_AFTER_DAYS`（例如，7天）內都沒有被更新過，它就會認為這是一個「冷」資料——即不經常被存取或修改的歷史數據。
3.  **自動轉換**：隨後，系統會呼叫 `migrate_baseline_format` 函數，將這個 `.lz4` 檔案解壓縮，然後再用壓縮率更高**的歸檔格式（如 `.zst`）重新壓縮。原始的 `.lz4` 檔案會被刪除。
4.  **透明化操作**：這個過程對使用者是完全透明的，您無需手動干預。

**結果**：常用的、頻繁變更的檔案保持高速存取（使用 LZ4），而那些不常用的、歷史性的檔案則以最高效率儲存（使用 Zstd），實現了儲存資源的**動態最優化**。這對於長期運行且監控大量 Excel 檔案的系統來說，可以顯著節省磁碟空間，同時不影響核心性能。

### 2.5 斷點續傳 (`ENABLE_RESUME`)：永不言棄的穩健性

在初始化階段，如果需要為成百上千個大型 Excel 檔案建立基準線，這個過程可能會持續數十分鐘甚至數小時。如果程式在這個過程中因為任何原因（如手動中斷、系統重啟、記憶體不足、網路斷開）而終止，從頭再來過無疑是無法接受的，這會極大地影響使用者體驗和工作效率。

`ENABLE_RESUME` 功能就是為了解決這個問題而設計的，它提供了一種**斷點續傳**的能力，確保即使程式意外終止，也能從上次中斷的地方繼續執行，而無需從頭開始。其實現機制既簡單又可靠：

1.  **進度記錄**：在 `create_baseline_for_files_robust` 函數的迴圈中，每成功處理完一個檔案，系統就會呼叫 `save_progress(i + 1, total)`。這個函數會將當前已經完成的檔案數量 (`i + 1`) 和總檔案數 (`total`) 寫入到一個指定的日誌檔案中 (`settings.RESUME_LOG_FILE`)。這個日誌檔案非常小，只記錄了進度信息。
2.  **啟動時檢查**：當程式下一次啟動並執行 `create_baseline_for_files_robust` 時，它會首先檢查 `settings.RESUME_LOG_FILE` 是否存在。
3.  **詢問與恢復**：如果進度檔案存在，系統會讀取其中記錄的 `completed` 數量，並在控制台詢問使用者是否要從上次中斷的地方繼續。如果使用者同意，迴圈的起始索引 `start_index` 就會被設定為 `completed` 的值，而不是從 0 開始。這意味著程式會跳過已經處理過的檔案，直接從未完成的部分開始。
4.  **完成後清理**：當所有檔案的基準線都成功建立後，系統會自動刪除這個進度檔案，為下一次全新的執行做好準備。這確保了每次完整的基準線建立都是一個「乾淨」的過程。

這種機制確保了基準線建立過程的**冪等性 (Idempotence)** 和**可恢復性 (Recoverability)**：

*   **冪等性**：無論執行多少次，只要輸入相同，結果都是一樣的。即使重複處理已經建立基準線的檔案，也不會產生錯誤或重複的基準線。
*   **可恢復性**：即使在處理過程中發生故障，系統也能從最近的檢查點恢復，避免了重複勞動。

這極大地提升了系統在處理大規模初始化任務時的健壯性和使用者體驗，讓您無需擔心長時間運行任務的中斷問題。